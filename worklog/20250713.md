本研究では、俳句データとして「https://haikudatabase.com/」に掲載されている作品を参考に、機械学習モデルの学習用データとして活用した。

遇到的问题：使用1万首俳句，最开始的词汇表word2id包含13345个词汇，词嵌入维度（embedding dim）设置为128，LSTM单元数（lstm_units）设置为256，每一句俳句的最大序列长度设置为（自动填充至20），最后的训练数据的次元是 (96697, 20)，目标数据的次元是(96697,)
构建的模型是Sequential模型，第一层是嵌入层 Embedding(vocab_size, embedding_dim, input_length=max_length)其中vocab_size=13345，embedding_dim=128， 	词向量层（13345个词 × 128维）
第二层 LSTM(lstm_units, return_sequences=True, dropout=0.2),通过return_sequences=True返回完整的序列而不是最后一个
第三层LSTM(lstm_units, dropout=0.2),
第四层全链接Dense(lstm_units, activation='relu'),激活函数为relu
第五层Dropout(0.3),
最后一层 第六层 全链接Dense(vocab_size, activation='softmax')
最后的可训练的总参数是5532065个

模型学习时的设定 オプティマイザー : Adam。損失関数 : スパースカテゴリカルクロスエントロピー。メトリック : 正解率(accuracy)
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

第一次学习了20轮，batch_size设置为64，验证数据设置为0.1
Epoch	Train Loss	Val Loss	Train Acc		Val Acc
   1		6.9685		6.3698	0.1288		0.1817
   4		5.6076		6.5261	0.2118		0.2072
   7		5.0176		7.0718	0.2377		0.2166
   8		4.8576		7.2930	0.2421		0.2161

第四轮开始出现了过拟合，在验证数据上的学习表现开始下降。之后几轮过拟合越来越严重。


为了改善过拟合，进行了如下的操作：
第一：在准备word2id时，进行了频率过滤（出现次数小于3的词自动纳入word2id["<UNK>"] = 1）将vocab_size从13345降低至4370，在保证训练有质量的同时降低了学习难度和过拟合的概率
第二：LSTM单元数从256减少至128，降低模型大小，最大序列长度设置从20降低至16（百分之99.99的数据都小于16）
第三：模型第二层和第三层的dropout从0.2增加至0.3，第五层的dropout层从0.3升至0.4
第四：加入早期停止（当val loss连续4次不下降就自动停止）和模型自动保存（只保存val loss最低的那一轮模型）
结果 总参数从5532065降低至3709345
训练结果 第五轮时模型训练达到最优


问题：俳句的语法错误百出，甚至会出现阿拉伯数字
解决对策：
增加原始数据集 从一万首俳句增加至两万首（视情况是否增加至三万？）
清洗原始数据集
LSTM层增加至3层
全句语法监督
引入预训练词向量
设置语言规则（避免“动词结尾+助词开头+名词堆叠”）
optimizer选择（目前用的adam梯度爆炸问题严重，没想好替换成什么优化器，但adam可能还是最优解）
重要：增加过滤系统 是否是17音？是否包含季语？是否只含一个切れ字？
明确避免助词起句/连续助词
季语信息的显式引入（在 Embedding 层后附加“季语”作为一个附加维度（例如通过 Concatenate））


参考的数据：米田ら 38,506句俳句  LSTM 层数 3层 学习率learning_rate=0.02  反向传播步长 BPTT_length = 100


参考文献
1. 米田 航紀，横山 想一郎，山下 倫央，川村 秀憲：
「LSTM を用いた俳句自動生成器の開発」，
人工知能学会全国大会論文集，vol.32，2018，1B2-OS-11b-01

2. 太田 玲子：
「深層学習による俳句の自動生成」，
奈良先端科学技術大学院大学 修士論文，NAIST-IS-MT1651023，2018年3月

3. 加藤 智一，竹田 晃人：
「季語情報付与による俳句自動生成器の改良と評価」，
言語処理学会 第28回年次大会 発表論文集，pp.927–929，2022年3月

4. 俳句データベースドットコム
「https://haikudatabase.com/」

