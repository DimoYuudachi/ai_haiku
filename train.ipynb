{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d156f12-2c88-457b-849c-9a4b99eba00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# MatplotlibãŠã‚ˆã³Seabornã§æ—¥æœ¬èªã‚’è¡¨ç¤ºå¯èƒ½ã«ã™ã‚‹\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'MS Gothic'\n",
    "\n",
    "# é«˜è§£åƒåº¦ãªPNGã§ã‚°ãƒ©ãƒ•ã‚’å‡ºåŠ›ã™ã‚‹\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "from flask import Flask\n",
    "print(\"Flaskå°å…¥ã•ã‚ŒãŸ\")\n",
    "\n",
    "import pyopenjtalk\n",
    "import pykakasi\n",
    "from sudachipy import tokenizer, dictionary\n",
    "import pyopenjtalk\n",
    "from collections import defaultdict, Counter\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "# è¯»å–ä½ åˆšæ‰ä¿å­˜çš„CSVæ–‡ä»¶\n",
    "df = pd.read_csv(\"dataset/processed_haiku.csv\", encoding=\"utf-8\")\n",
    "print(df.head())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"=== ç¬¬2æ­¥ï¼šå‡†å¤‡word2idå­—å…¸ï¼ˆå«é¢‘ç‡è¿‡æ»¤ï¼‰ ===\")\n",
    "\n",
    "# ç»Ÿè®¡æ‰€æœ‰ token çš„é¢‘ç‡\n",
    "token_counter = Counter()\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        tokens = ast.literal_eval(row['Tokens']) if isinstance(row['Tokens'], str) else row['Tokens']\n",
    "        token_counter.update(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç†ç¬¬{idx}è¡Œæ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "# è®¾ç½®æœ€å°è¯é¢‘\n",
    "min_freq = 3\n",
    "print(f\"ä»…ä¿ç•™å‡ºç° â‰¥ {min_freq} æ¬¡çš„è¯\")\n",
    "\n",
    "# åˆå§‹åŒ– word2id\n",
    "word2id = defaultdict(lambda: word2id[\"<UNK>\"])\n",
    "word2id[\"<PAD>\"] = 0\n",
    "word2id[\"<UNK>\"] = 1\n",
    "word2id[\"<START>\"] = 2\n",
    "word2id[\"<END>\"] = 3\n",
    "\n",
    "# åªåŠ å…¥é¢‘ç‡ >= min_freq çš„è¯\n",
    "for token, freq in token_counter.items():\n",
    "    if freq >= min_freq:\n",
    "        word2id[token] = len(word2id)\n",
    "\n",
    "print(f\"è¯æ±‡è¡¨å¤§å°ï¼ˆè¿‡æ»¤åï¼‰: {len(word2id)}\")\n",
    "print(f\"å‰10ä¸ªè¯æ±‡:\")\n",
    "for i, (word, id) in enumerate(word2id.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"  {word} -> {id}\")\n",
    "\n",
    "print(\"\\nâœ… ç¬¬2æ­¥å®Œæˆï¼è¯æ±‡è¡¨å·²å‡†å¤‡å¥½ï¼ˆç¨€æœ‰è¯è‡ªåŠ¨æ˜ å°„ä¸º <UNK>ï¼‰\")\n",
    "\n",
    "with open(\"dataset/word2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict(word2id), f)\n",
    "\n",
    "# ä¿å­˜ id2wordï¼ˆå¯ç”¨äºç”Ÿæˆæ—¶æŠŠ id è½¬æˆè¯ï¼‰\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "with open(\"dataset/id2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2word, f)\n",
    "\n",
    "# ç¬¬3æ­¥ï¼šè½¬æ¢IDæ•°æ®\n",
    "print(\"=== ç¬¬3æ­¥ï¼šè½¬æ¢IDæ•°æ® ===\")\n",
    "\n",
    "haiku_ids_list = []\n",
    "error_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        # è§£æåˆ†è¯ç»“æœ\n",
    "        if isinstance(row['Tokens'], str):\n",
    "            tokens = ast.literal_eval(row['Tokens'])\n",
    "        else:\n",
    "            tokens = row['Tokens']\n",
    "        \n",
    "        # è½¬æ¢ä¸ºIDåºåˆ—\n",
    "        ids = [word2id[token] for token in tokens]\n",
    "        haiku_ids_list.append(ids)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        if error_count <= 3:\n",
    "            print(f\"å¤„ç†ç¬¬{idx}è¡Œæ—¶å‡ºé”™: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"æˆåŠŸè½¬æ¢ {len(haiku_ids_list)} é¦–ä¿³å¥\")\n",
    "print(f\"é”™è¯¯è¡Œæ•°: {error_count}\")\n",
    "\n",
    "# æ£€æŸ¥IDåºåˆ—çš„é•¿åº¦åˆ†å¸ƒ\n",
    "lengths = [len(ids) for ids in haiku_ids_list]\n",
    "print(f\"\\nä¿³å¥é•¿åº¦ç»Ÿè®¡:\")\n",
    "print(f\"  æœ€çŸ­: {min(lengths)} ä¸ªè¯\")\n",
    "print(f\"  æœ€é•¿: {max(lengths)} ä¸ªè¯\")\n",
    "print(f\"  å¹³å‡: {sum(lengths)/len(lengths):.1f} ä¸ªè¯\")\n",
    "\n",
    "# æ˜¾ç¤ºå‡ ä¸ªä¾‹å­\n",
    "print(\"\\n=== IDè½¬æ¢ä¾‹å­ ===\")\n",
    "for i in range(min(3, len(haiku_ids_list))):\n",
    "    print(f\"\\nä¾‹å­ {i+1}:\")\n",
    "    print(f\"åŸå§‹ä¿³å¥: {df.iloc[i]['Haiku']}\")\n",
    "    print(f\"IDåºåˆ—: {haiku_ids_list[i]}\")\n",
    "    print(f\"é•¿åº¦: {len(haiku_ids_list[i])}\")\n",
    "\n",
    "print(\"\\nâœ… ç¬¬3æ­¥å®Œæˆï¼ç°åœ¨ä½ æœ‰äº†:\")\n",
    "print(f\"1. è¯æ±‡è¡¨ word2idï¼Œå¤§å°: {len(word2id)}\")\n",
    "print(f\"2. IDåºåˆ—åˆ—è¡¨ haiku_ids_listï¼ŒåŒ…å« {len(haiku_ids_list)} é¦–ä¿³å¥\")\n",
    "print(\"\\nğŸ‰ å‡†å¤‡è¿›å…¥ä¸‹ä¸€æ­¥ï¼šæ„å»ºLSTMæ¨¡å‹\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"=== ç¬¬4aæ­¥ï¼šè®¾ç½®å‚æ•°å’Œå‡†å¤‡è®­ç»ƒæ•°æ® ===\")\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# æ¨¡å‹å‚æ•°\n",
    "vocab_size = len(word2id)\n",
    "embedding_dim = 128      # è¯åµŒå…¥ç»´åº¦\n",
    "lstm_units = 128         # LSTMå•å…ƒæ•°\n",
    "max_length = 16          # æœ€å¤§åºåˆ—é•¿åº¦\n",
    "\n",
    "print(f\"æ¨¡å‹å‚æ•°:\")\n",
    "print(f\"  è¯æ±‡è¡¨å¤§å°: {vocab_size}\")\n",
    "print(f\"  åµŒå…¥ç»´åº¦: {embedding_dim}\")\n",
    "print(f\"  LSTMå•å…ƒæ•°: {lstm_units}\")\n",
    "print(f\"  æœ€å¤§åºåˆ—é•¿åº¦: {max_length}\")\n",
    "print(f\"  <START>æ ‡è®°ID: {word2id['<START>']}\")\n",
    "print(f\"  <END>æ ‡è®°ID: {word2id['<END>']}\")\n",
    "\n",
    "print(\"\\nâœ… ç¬¬4aæ­¥å®Œæˆï¼å‚æ•°è®¾ç½®å®Œæ¯•\")\n",
    "\n",
    "# ç¬¬4bæ­¥ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "print(\"=== ç¬¬4bæ­¥ï¼šå‡†å¤‡è®­ç»ƒæ•°æ® ===\")\n",
    "\n",
    "def prepare_training_data(haiku_ids_list, max_length):\n",
    "    \"\"\"å°†ä¿³å¥IDåºåˆ—è½¬æ¢ä¸ºè®­ç»ƒæ•°æ®\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for haiku_ids in haiku_ids_list:\n",
    "        # ä¸ºæ¯é¦–ä¿³å¥æ·»åŠ å¼€å§‹å’Œç»“æŸæ ‡è®°\n",
    "        sequence = [word2id[\"<START>\"]] + haiku_ids + [word2id[\"<END>\"]]\n",
    "        \n",
    "        # åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ—\n",
    "        for i in range(1, len(sequence)):\n",
    "            input_seq = sequence[:i]\n",
    "            target = sequence[i]\n",
    "            \n",
    "            # åªå¤„ç†ä¸å¤ªé•¿çš„åºåˆ—\n",
    "            if len(input_seq) <= max_length:\n",
    "                X.append(input_seq)\n",
    "                y.append(target)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print(\"å‡†å¤‡è®­ç»ƒæ•°æ®...\")\n",
    "X, y = prepare_training_data(haiku_ids_list, max_length)\n",
    "\n",
    "print(f\"è®­ç»ƒæ ·æœ¬æ•°: {len(X)}\")\n",
    "print(f\"ç›®æ ‡æ ·æœ¬æ•°: {len(y)}\")\n",
    "\n",
    "# æ˜¾ç¤ºå‡ ä¸ªè®­ç»ƒæ ·æœ¬\n",
    "print(\"\\n=== è®­ç»ƒæ•°æ®ç¤ºä¾‹ ===\")\n",
    "for i in range(min(3, len(X))):\n",
    "    print(f\"æ ·æœ¬ {i+1}:\")\n",
    "    print(f\"  è¾“å…¥åºåˆ—: {X[i]}\")\n",
    "    print(f\"  ç›®æ ‡è¯ID: {y[i]}\")\n",
    "    print(f\"  è¾“å…¥é•¿åº¦: {len(X[i])}\")\n",
    "\n",
    "print(\"\\nâœ… ç¬¬4bæ­¥å®Œæˆï¼è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæ¯•\")\n",
    "\n",
    "# ç¬¬4cæ­¥ï¼šæ•°æ®å¡«å……\n",
    "print(\"=== ç¬¬4cæ­¥ï¼šæ•°æ®å¡«å…… ===\")\n",
    "\n",
    "# å¡«å……åºåˆ—åˆ°ç›¸åŒé•¿åº¦\n",
    "print(f\"å¡«å……åºåˆ—åˆ°é•¿åº¦ {max_length}...\")\n",
    "X_padded = pad_sequences(X, maxlen=max_length, padding='pre')\n",
    "y_array = np.array(y)\n",
    "\n",
    "print(f\"å¡«å……åçš„è¾“å…¥å½¢çŠ¶: {X_padded.shape}\")\n",
    "print(f\"ç›®æ ‡æ•°æ®å½¢çŠ¶: {y_array.shape}\")\n",
    "\n",
    "# æ˜¾ç¤ºå¡«å……å‰åçš„å¯¹æ¯”\n",
    "print(\"\\n=== å¡«å……å‰åå¯¹æ¯” ===\")\n",
    "print(f\"å¡«å……å‰ç¬¬1ä¸ªæ ·æœ¬: {X[0]}\")\n",
    "print(f\"å¡«å……åç¬¬1ä¸ªæ ·æœ¬: {X_padded[0]}\")\n",
    "print(f\"å¯¹åº”ç›®æ ‡: {y_array[0]}\")\n",
    "\n",
    "# æŸ¥çœ‹ä¸€äº›ç»Ÿè®¡ä¿¡æ¯\n",
    "print(f\"\\n=== æ•°æ®ç»Ÿè®¡ ===\")\n",
    "print(f\"è¾“å…¥åºåˆ—æœ€å°å€¼: {X_padded.min()}\")\n",
    "print(f\"è¾“å…¥åºåˆ—æœ€å¤§å€¼: {X_padded.max()}\")\n",
    "print(f\"ç›®æ ‡è¯IDæœ€å°å€¼: {y_array.min()}\")\n",
    "print(f\"ç›®æ ‡è¯IDæœ€å¤§å€¼: {y_array.max()}\")\n",
    "\n",
    "print(\"\\nâœ… ç¬¬4cæ­¥å®Œæˆï¼æ•°æ®å¡«å……å®Œæ¯•\")\n",
    "\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    LSTM(lstm_units, return_sequences=True, dropout=0.3),\n",
    "    LSTM(lstm_units, dropout=0.3),\n",
    "    Dense(lstm_units, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "#ã€€ãƒ¢ãƒ‡ãƒ«ã®è¦ç´„\n",
    "model.build(input_shape=(None, max_length))\n",
    "model.summary()\n",
    "\n",
    "# å­¦ç¿’æ™‚ã®è¨­å®š\n",
    "# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ : Adam\n",
    "# æå¤±é–¢æ•° : ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼\n",
    "# ãƒ¡ãƒˆãƒªãƒƒã‚¯ : æ­£è§£ç‡(accuracy)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# æ—©æœŸåœæ­¢ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=4,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ä¿å­˜ï¼ˆåªä¿å­˜éªŒè¯é›†ä¸Šæœ€ä¼˜çš„æ¨¡å‹ï¼‰\n",
    "model_ckpt = ModelCheckpoint(\n",
    "    filepath='model/best_model.keras',       # ä¿å­˜è·¯å¾„ï¼ˆå»ºè®®ç”¨ .kerasï¼‰\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,              # åªä¿å­˜æœ€å¥½çš„é‚£ä¸€è½®\n",
    "    mode='min',                       # val_loss è¶Šå°è¶Šå¥½\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# å­¦ç¿’\n",
    "epoch = 20\n",
    "hist = model.fit(X_padded, y_array, epochs=epoch, batch_size=64, validation_split=0.1, callbacks=[early_stop, model_ckpt])\n",
    "\n",
    "model.save(\"model/final_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
