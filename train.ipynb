{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d156f12-2c88-457b-849c-9a4b99eba00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# MatplotlibおよびSeabornで日本語を表示可能にする\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'MS Gothic'\n",
    "\n",
    "# 高解像度なPNGでグラフを出力する\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "from flask import Flask\n",
    "print(\"Flask導入された\")\n",
    "\n",
    "import pyopenjtalk\n",
    "import pykakasi\n",
    "from sudachipy import tokenizer, dictionary\n",
    "import pyopenjtalk\n",
    "from collections import defaultdict, Counter\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "# 读取你刚才保存的CSV文件\n",
    "df = pd.read_csv(\"dataset/processed_haiku.csv\", encoding=\"utf-8\")\n",
    "print(df.head())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"=== 第2步：准备word2id字典（含频率过滤） ===\")\n",
    "\n",
    "# 统计所有 token 的频率\n",
    "token_counter = Counter()\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        tokens = ast.literal_eval(row['Tokens']) if isinstance(row['Tokens'], str) else row['Tokens']\n",
    "        token_counter.update(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"处理第{idx}行时出错: {e}\")\n",
    "\n",
    "# 设置最小词频\n",
    "min_freq = 3\n",
    "print(f\"仅保留出现 ≥ {min_freq} 次的词\")\n",
    "\n",
    "# 初始化 word2id\n",
    "word2id = defaultdict(lambda: word2id[\"<UNK>\"])\n",
    "word2id[\"<PAD>\"] = 0\n",
    "word2id[\"<UNK>\"] = 1\n",
    "word2id[\"<START>\"] = 2\n",
    "word2id[\"<END>\"] = 3\n",
    "\n",
    "# 只加入频率 >= min_freq 的词\n",
    "for token, freq in token_counter.items():\n",
    "    if freq >= min_freq:\n",
    "        word2id[token] = len(word2id)\n",
    "\n",
    "print(f\"词汇表大小（过滤后）: {len(word2id)}\")\n",
    "print(f\"前10个词汇:\")\n",
    "for i, (word, id) in enumerate(word2id.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"  {word} -> {id}\")\n",
    "\n",
    "print(\"\\n✅ 第2步完成！词汇表已准备好（稀有词自动映射为 <UNK>）\")\n",
    "\n",
    "with open(\"dataset/word2id.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dict(word2id), f)\n",
    "\n",
    "# 保存 id2word（可用于生成时把 id 转成词）\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "with open(\"dataset/id2word.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id2word, f)\n",
    "\n",
    "# 第3步：转换ID数据\n",
    "print(\"=== 第3步：转换ID数据 ===\")\n",
    "\n",
    "haiku_ids_list = []\n",
    "error_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        # 解析分词结果\n",
    "        if isinstance(row['Tokens'], str):\n",
    "            tokens = ast.literal_eval(row['Tokens'])\n",
    "        else:\n",
    "            tokens = row['Tokens']\n",
    "        \n",
    "        # 转换为ID序列\n",
    "        ids = [word2id[token] for token in tokens]\n",
    "        haiku_ids_list.append(ids)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        if error_count <= 3:\n",
    "            print(f\"处理第{idx}行时出错: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"成功转换 {len(haiku_ids_list)} 首俳句\")\n",
    "print(f\"错误行数: {error_count}\")\n",
    "\n",
    "# 检查ID序列的长度分布\n",
    "lengths = [len(ids) for ids in haiku_ids_list]\n",
    "print(f\"\\n俳句长度统计:\")\n",
    "print(f\"  最短: {min(lengths)} 个词\")\n",
    "print(f\"  最长: {max(lengths)} 个词\")\n",
    "print(f\"  平均: {sum(lengths)/len(lengths):.1f} 个词\")\n",
    "\n",
    "# 显示几个例子\n",
    "print(\"\\n=== ID转换例子 ===\")\n",
    "for i in range(min(3, len(haiku_ids_list))):\n",
    "    print(f\"\\n例子 {i+1}:\")\n",
    "    print(f\"原始俳句: {df.iloc[i]['Haiku']}\")\n",
    "    print(f\"ID序列: {haiku_ids_list[i]}\")\n",
    "    print(f\"长度: {len(haiku_ids_list[i])}\")\n",
    "\n",
    "print(\"\\n✅ 第3步完成！现在你有了:\")\n",
    "print(f\"1. 词汇表 word2id，大小: {len(word2id)}\")\n",
    "print(f\"2. ID序列列表 haiku_ids_list，包含 {len(haiku_ids_list)} 首俳句\")\n",
    "print(\"\\n🎉 准备进入下一步：构建LSTM模型\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"=== 第4a步：设置参数和准备训练数据 ===\")\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 模型参数\n",
    "vocab_size = len(word2id)\n",
    "embedding_dim = 128      # 词嵌入维度\n",
    "lstm_units = 128         # LSTM单元数\n",
    "max_length = 16          # 最大序列长度\n",
    "\n",
    "print(f\"模型参数:\")\n",
    "print(f\"  词汇表大小: {vocab_size}\")\n",
    "print(f\"  嵌入维度: {embedding_dim}\")\n",
    "print(f\"  LSTM单元数: {lstm_units}\")\n",
    "print(f\"  最大序列长度: {max_length}\")\n",
    "print(f\"  <START>标记ID: {word2id['<START>']}\")\n",
    "print(f\"  <END>标记ID: {word2id['<END>']}\")\n",
    "\n",
    "print(\"\\n✅ 第4a步完成！参数设置完毕\")\n",
    "\n",
    "# 第4b步：准备训练数据\n",
    "print(\"=== 第4b步：准备训练数据 ===\")\n",
    "\n",
    "def prepare_training_data(haiku_ids_list, max_length):\n",
    "    \"\"\"将俳句ID序列转换为训练数据\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for haiku_ids in haiku_ids_list:\n",
    "        # 为每首俳句添加开始和结束标记\n",
    "        sequence = [word2id[\"<START>\"]] + haiku_ids + [word2id[\"<END>\"]]\n",
    "        \n",
    "        # 创建滑动窗口序列\n",
    "        for i in range(1, len(sequence)):\n",
    "            input_seq = sequence[:i]\n",
    "            target = sequence[i]\n",
    "            \n",
    "            # 只处理不太长的序列\n",
    "            if len(input_seq) <= max_length:\n",
    "                X.append(input_seq)\n",
    "                y.append(target)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print(\"准备训练数据...\")\n",
    "X, y = prepare_training_data(haiku_ids_list, max_length)\n",
    "\n",
    "print(f\"训练样本数: {len(X)}\")\n",
    "print(f\"目标样本数: {len(y)}\")\n",
    "\n",
    "# 显示几个训练样本\n",
    "print(\"\\n=== 训练数据示例 ===\")\n",
    "for i in range(min(3, len(X))):\n",
    "    print(f\"样本 {i+1}:\")\n",
    "    print(f\"  输入序列: {X[i]}\")\n",
    "    print(f\"  目标词ID: {y[i]}\")\n",
    "    print(f\"  输入长度: {len(X[i])}\")\n",
    "\n",
    "print(\"\\n✅ 第4b步完成！训练数据准备完毕\")\n",
    "\n",
    "# 第4c步：数据填充\n",
    "print(\"=== 第4c步：数据填充 ===\")\n",
    "\n",
    "# 填充序列到相同长度\n",
    "print(f\"填充序列到长度 {max_length}...\")\n",
    "X_padded = pad_sequences(X, maxlen=max_length, padding='pre')\n",
    "y_array = np.array(y)\n",
    "\n",
    "print(f\"填充后的输入形状: {X_padded.shape}\")\n",
    "print(f\"目标数据形状: {y_array.shape}\")\n",
    "\n",
    "# 显示填充前后的对比\n",
    "print(\"\\n=== 填充前后对比 ===\")\n",
    "print(f\"填充前第1个样本: {X[0]}\")\n",
    "print(f\"填充后第1个样本: {X_padded[0]}\")\n",
    "print(f\"对应目标: {y_array[0]}\")\n",
    "\n",
    "# 查看一些统计信息\n",
    "print(f\"\\n=== 数据统计 ===\")\n",
    "print(f\"输入序列最小值: {X_padded.min()}\")\n",
    "print(f\"输入序列最大值: {X_padded.max()}\")\n",
    "print(f\"目标词ID最小值: {y_array.min()}\")\n",
    "print(f\"目标词ID最大值: {y_array.max()}\")\n",
    "\n",
    "print(\"\\n✅ 第4c步完成！数据填充完毕\")\n",
    "\n",
    "\n",
    "# モデルの作成\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    LSTM(lstm_units, return_sequences=True, dropout=0.3),\n",
    "    LSTM(lstm_units, dropout=0.3),\n",
    "    Dense(lstm_units, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "#　モデルの要約\n",
    "model.build(input_shape=(None, max_length))\n",
    "model.summary()\n",
    "\n",
    "# 学習時の設定\n",
    "# オプティマイザー : Adam\n",
    "# 損失関数 : スパースカテゴリカルクロスエントロピー\n",
    "# メトリック : 正解率(accuracy)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# 早期停止（防止过拟合）\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=4,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# モデルの自動保存（只保存验证集上最优的模型）\n",
    "model_ckpt = ModelCheckpoint(\n",
    "    filepath='model/best_model.keras',       # 保存路径（建议用 .keras）\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,              # 只保存最好的那一轮\n",
    "    mode='min',                       # val_loss 越小越好\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 学習\n",
    "epoch = 20\n",
    "hist = model.fit(X_padded, y_array, epochs=epoch, batch_size=64, validation_split=0.1, callbacks=[early_stop, model_ckpt])\n",
    "\n",
    "model.save(\"model/final_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
